{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from importlib import reload\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import utils.plots_cliffwalking as plots\n",
    "from env.cliff_walking import WindyCliffWalking\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Cliff Walking Environment with Deep Q-Learning\n",
    "\n",
    "The [Cliff Walking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) environment will be used again to understand how Deep Q-Learning, the simplest Deep Reinforcement Learning algorithm, works. Recapping the objective of Cliff Walking: the agent must traverse a grid from start to finish while avoiding falling off a cliff. If the agent falls off the cliff, it returns to the start of the grid and receives a penalty.\n",
    "\n",
    "<img src=\"media/cliff_walking.gif\" width=\"200\">\n",
    "\n",
    "Below are some important details for modeling the environment as a Markov Decision Process (MDP):\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The action space is discrete and contains integers in the range {0, 3}. An action specifies the direction of a movement:\n",
    "* 0: Up\n",
    "* 1: Right\n",
    "* 2: Down\n",
    "* 3: Left\n",
    "\n",
    "### State Space\n",
    "\n",
    "The state represents the agent's position on the grid. Thus, the state space is also discrete and contains integers in the range {0, 47}. The numeric value of the agent's position on the grid can be obtained as `current_row * n_rows + column`, with rows and columns starting at 0.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "For each movement the agent makes, a penalty of -1 is applied, unless the agent falls off the cliff, which results in a penalty of -100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "In Deep Q-Learning, the Q-values of each action associated with a state are calculated using a neural network. In other words, the neural network takes a state vector as input and returns a Q-value vector, where each element represents the Q-value of an action. A Q-value can be interpreted as \"the expected total accumulated reward for taking action A in state S and then following the same policy until the end of the episode.\"\n",
    "\n",
    "Since this is a relatively simple problem, the model used to solve Cliff Walking can be an MLP (Multi-Layer Perceptron) network. Additionally, note that predicting Q-values is a regression task, so a softmax activation function is not used at the end of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer to (x, y) Conversion\n",
    "\n",
    "The neural network becomes simpler if it receives a pair (x, y) as input instead of an integer. If we were to feed it an integer, we would need to convert that integer to a one-hot encoding. Since there are 48 states, this would require a 48-element vector with 47 zeroed elements! By passing a pair (x, y), the vector is reduced to just two elements. The method `_encode_state()` encodes the state as the agent's position and transforms it into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, layer_sizes: List[int] = [64, 64]):\n",
    "        super().__init__()\n",
    "\n",
    "        # construindo a rede neural\n",
    "        layers = []\n",
    "        input_size = 2 # entrada: posicao (x, y)\n",
    "        for n_neurons in layer_sizes:\n",
    "            layers.append(nn.Linear(input_size, n_neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = n_neurons\n",
    "        layers.append(nn.Linear(input_size, 4))\n",
    "        self.nn = nn.Sequential(*layers).to(DEVICE)\n",
    "\n",
    "    def _encode_state(self, state: int | torch.Tensor) -> torch.Tensor:\n",
    "        if isinstance(state, int):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        x = (state % 12) / 11\n",
    "        y = (state // 12) / 3\n",
    "\n",
    "        x = x.reshape(-1, 1)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        return torch.cat([x, y], dim=-1).to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._encode_state(x)\n",
    "        x = self.nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "Replay buffers are used to store state transitions observed during the training process. The neural network uses batches of transitions (not necessarily sequential!) to calculate the loss and update its weights. Using replay buffers makes the learning process more efficient by allowing the network to learn from a transition multiple times and by helping to improve the stability of neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'terminated'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=1024):\n",
    "        self.buffer = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\" Save a transition into the buffer.\"\"\"\n",
    "        self.buffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Actions with the $\\epsilon$-Greedy Policy\n",
    "\n",
    "At the end of training, the best action for each state is expected to be the one with the highest Q-value. However, for Q-Learning to converge properly, the agent must \"explore\" the environment thoroughly at the beginning of training. This means the agent should visit a large number of states, even if they are not necessarily optimal. A widely used technique for this purpose is the $\\epsilon$-greedy policy. It forces the agent to choose actions randomly with a frequency that decreases as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_action(model, state, epsilon, random=True, n_actions=4):\n",
    "    if torch.rand(1) < epsilon and random:\n",
    "        return torch.randint(n_actions, (1,)).item()\n",
    "    qvals = model(state)\n",
    "    return torch.argmax(qvals).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training \n",
    "\n",
    "At each training step, the agent will perform an action and use the information returned by the environment to calculate a loss and update its weights to minimize it. The loss used will be the mean squared error between the selected Q-value and the highest Q-value of the next state, calculated using the network with weights from the previous update:\n",
    "\n",
    "$$L_i(\\theta_i)=\\mathbb{E}[(y_i - Q(s,a;\\theta_i))^2]$$  \n",
    "$$y_i=\\mathbb{E}[R(s')+\\gamma\\max_A Q(s',A;\\theta_{i-1})]$$  \n",
    "\n",
    "Note that, therefore, two neural networks with the same architecture will be required, but one will have weights delayed by one iteration compared to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_net(model: Qnet, \n",
    "                 model_target: Qnet,\n",
    "                 optimizer: torch.optim.Optimizer, \n",
    "                 batch_of_transitions: List[Transition],\n",
    "                 gamma):\n",
    "    \n",
    "    # Convert a list of Transitions into a Transition of lists\n",
    "    # Check: https://stackoverflow.com/questions/19339/transpose-unzip-function-inverse-of-zip/19343#19343\n",
    "    batch = Transition(*zip(*batch_of_transitions))\n",
    "    \n",
    "    # transforming the state batch into a tensor\n",
    "    states = torch.tensor(batch.state, dtype=torch.float32)\n",
    "    actions = torch.tensor(batch.action, dtype=torch.int64).to(DEVICE)\n",
    "    rewards = torch.tensor(batch.reward, dtype=torch.float32).to(DEVICE)\n",
    "    next_states = torch.tensor(batch.next_state, dtype=torch.float32)\n",
    "    terminated = torch.tensor(batch.terminated, dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    predictions = model(states).gather(1, actions.unsqueeze(1)).squeeze() # select the Q(s, a) for the actions taken\n",
    "\n",
    "    # if in the final state, the expected value is the reward itself\n",
    "    with torch.no_grad():\n",
    "        targets = rewards + gamma * model_target(next_states).max(-1).values * (1 - terminated.int()) # else rewards + gamma * max_a Q(s', a)\n",
    "    \n",
    "    loss = F.mse_loss(predictions, targets)\n",
    "\n",
    "    # update the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "In the training loop, we will combine all the functions developed so far. The main idea is to define a maximum number of episodes (from the initial stage to the final stage) so that the agent can gather experiences from the environment and optimize its Q-value table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Heatmap:\n",
    "    def __init__(self, rows, cols):\n",
    "        self.heatmap = torch.zeros(rows, cols)\n",
    "    \n",
    "    def update(self, state):\n",
    "        col = state % 12\n",
    "        row = state // 12\n",
    "        self.heatmap[row][col] += 1\n",
    "\n",
    "    def clear(self):\n",
    "        self.heatmap = torch.zeros(self.heatmap.shape)\n",
    "    \n",
    "    def plot(self, episode):\n",
    "        print(self.heatmap.int())\n",
    "        plt.imshow(self.heatmap, cmap='hot', interpolation='nearest')\n",
    "        plt.title(f'Heatmap for the last 20 eps from ep {episode}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(metrics: Dict, show_results=False):\n",
    "    rewards = torch.tensor(metrics['episode_rewards'])\n",
    "    plt.figure(1)\n",
    "    plt.clf\n",
    "    plt.title('Total reward of each episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total reward')\n",
    "    plt.plot(rewards)\n",
    "\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if metrics['avg_reward'] is not None: \n",
    "        x = range(49, 49 + len(metrics['avg_reward']))\n",
    "        plt.plot(x, metrics['avg_reward'].numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    # plot heatmap\n",
    "    metrics['heatmap'].plot(len(metrics['episode_rewards']))\n",
    "\n",
    "    # dump qtable\n",
    "    for i, q in enumerate(metrics['qtable']):\n",
    "        if i not in range(37, 47):\n",
    "            print(f'Q values for state {i}: {q}')\n",
    "\n",
    "    if not show_results:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        env: gym.Env, \n",
    "        model: Qnet,\n",
    "        total_steps=35_000,\n",
    "        replay_buffer_size=10_000, # replay buffer size\n",
    "        batch_size=64, # batch size\n",
    "        gamma=0.99,\n",
    "        learning_rate=1e-4,\n",
    "        learning_freq=1,\n",
    "        target_update_freq=100,\n",
    "        tau=1.0,\n",
    "        epsilon_0 = 1, # initial probability of taking a random action\n",
    "        epsilon_f=0.05, # final probability of taking a random action (after decay)\n",
    "        epsilon_decay = 100_000 # step in which epsilon will be approximately epsilon_f + 0.36 * (epsilon_0 - epsilon_f)\n",
    "        ):\n",
    "    \n",
    "    model_target = deepcopy(model)\n",
    "\n",
    "    def eps_scheduler(step):\n",
    "        return epsilon_f + (epsilon_0 - epsilon_f) * math.exp(-1. * step / epsilon_decay)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)    \n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    metrics = {\n",
    "        'episode_rewards': [],\n",
    "        'avg_reward': None,\n",
    "        'heatmap': Heatmap(4, 12),\n",
    "    }\n",
    "    episode_step = 0\n",
    "    truncated = False\n",
    "\n",
    "    for global_step in range(total_steps):\n",
    "        epsilon = eps_scheduler(global_step)\n",
    "\n",
    "        # observe\n",
    "        action = get_action(model, state, epsilon)\n",
    "        next_state, reward, terminated, _, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "        metrics['heatmap'].update(state)\n",
    "        total_reward += reward\n",
    "        episode_step += 1\n",
    "        if episode_step > 1000:\n",
    "            truncated = True\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # update\n",
    "        if global_step > batch_size and global_step % learning_freq == 0:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            model = update_q_net(model, model_target, optimizer, batch, gamma)\n",
    "\n",
    "        # update target network\n",
    "        if global_step % target_update_freq == 0:\n",
    "            model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "        # evaluating\n",
    "        if done:\n",
    "            metrics['episode_rewards'].append(total_reward)\n",
    "            if len(metrics['episode_rewards']) > 50:\n",
    "                metrics['avg_reward'] = torch.tensor(metrics['episode_rewards']).float().unfold(0, 50, 1).mean(1)\n",
    "                print(f'avg reward: {metrics[\"avg_reward\"][-1]}')\n",
    "            with torch.no_grad():\n",
    "                metrics['qtable'] = model(torch.arange(48))\n",
    "            print(f'current step: {global_step}, epsilon: {epsilon:.2f}')\n",
    "            evaluate(metrics)\n",
    "            total_reward = 0\n",
    "            metrics['heatmap'].clear()\n",
    "            state, _ = env.reset()\n",
    "            episode_step = 0\n",
    "            truncated = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Everything is set up, so now we can run the algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QnetEmb(nn.Module):\n",
    "    def __init__(self, n_actions=4):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Embedding(48, n_actions).to(DEVICE)\n",
    "\n",
    "    def _encode_state(self, state: int | torch.Tensor) -> torch.Tensor:\n",
    "        return torch.tensor(state).long().to(DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._encode_state(x)\n",
    "        x = self.nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m q_net \u001b[38;5;241m=\u001b[39m Qnet()\n\u001b[1;32m      3\u001b[0m debug_model \u001b[38;5;241m=\u001b[39m deepcopy(q_net)\n\u001b[0;32m----> 4\u001b[0m trained_q_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcliff_walking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, model, total_steps, replay_buffer_size, batch_size, gamma, learning_rate, learning_freq, target_update_freq, tau, epsilon_0, epsilon_f, epsilon_decay, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m>\u001b[39m batch_size \u001b[38;5;129;01mand\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m learning_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     batch \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[0;32m---> 52\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_q_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# update target network\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m target_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mupdate_q_net\u001b[0;34m(model, model_target, optimizer, batch_of_transitions, gamma)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# atualizando os pesos da rede\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cliff_walking = gym.make('CliffWalking-v0')\n",
    "q_net = Qnet()\n",
    "debug_model = deepcopy(q_net)\n",
    "trained_q_net = train(cliff_walking, q_net, epsilon_decay=10_000, learning_rate=1e-3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the agent\n",
    "\n",
    "The function below will run an episode with the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(env: \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mEnv, \n\u001b[1;32m      2\u001b[0m           q_net,\n\u001b[1;32m      3\u001b[0m           n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m           ):\n\u001b[1;32m      7\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "def test(env: gym.Env, \n",
    "          q_net,\n",
    "          n_episodes=1,\n",
    "          verbose=False\n",
    "          ):\n",
    "    \n",
    "    total_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = get_action(q_net, state, 0)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Episode {episode} - Total reward: {total_reward}\")\n",
    "            \n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return torch.mean(total_reward)\n",
    "\n",
    "test(gym.make('CliffWalking-v0', render_mode=\"human\"), trained_q_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "### Visualizing the Policy\n",
    "\n",
    "The cell below will allow observing the most likely action to be taken in each position on the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24739/1860028285.py:20: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  y = (state // 12) / 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m reload(plots)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_arrows_from_qnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/rl_practice_eldorado/utils/plots_cliffwalking.py:64\u001b[0m, in \u001b[0;36mplot_arrows_from_qnet\u001b[0;34m(q_net)\u001b[0m\n\u001b[1;32m     61\u001b[0m normalized_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(qvals\u001b[38;5;241m/\u001b[39mT, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# cima\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_q_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# direita\u001b[39;00m\n\u001b[1;32m     66\u001b[0m plt\u001b[38;5;241m.\u001b[39marrow(x, y, normalized_q_values[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m0\u001b[39m, head_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, head_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, fc\u001b[38;5;241m=\u001b[39ma, ec\u001b[38;5;241m=\u001b[39ma)\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/pyplot.py:2347\u001b[0m, in \u001b[0;36marrow\u001b[0;34m(x, y, dx, dy, **kwargs)\u001b[0m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39marrow)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marrow\u001b[39m(x, y, dx, dy, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4961\u001b[0m, in \u001b[0;36mAxes.arrow\u001b[0;34m(self, x, y, dx, dy, **kwargs)\u001b[0m\n\u001b[1;32m   4958\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_xunits(dx)\n\u001b[1;32m   4959\u001b[0m dy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits(dy)\n\u001b[0;32m-> 4961\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mmpatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFancyArrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_patch(a)\n\u001b[1;32m   4963\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_autoscale_view()\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/patches.py:1350\u001b[0m, in \u001b[0;36mFancyArrow.__init__\u001b[0;34m(self, x, y, dx, dy, width, length_includes_head, head_width, head_length, shape, overhang, head_starts_at_zero, **kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overhang \u001b[38;5;241m=\u001b[39m overhang\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_head_starts_at_zero \u001b[38;5;241m=\u001b[39m head_starts_at_zero\n\u001b[0;32m-> 1350\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_verts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverts, closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cleanrl/lib/python3.10/site-packages/matplotlib/patches.py:1409\u001b[0m, in \u001b[0;36mFancyArrow._make_verts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1408\u001b[0m     length \u001b[38;5;241m=\u001b[39m distance \u001b[38;5;241m+\u001b[39m head_length\n\u001b[0;32m-> 1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m length:\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# display nothing if empty\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;66;03m# start by drawing horizontal arrow, point at (0, 0)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(plots)\n",
    "plots.plot_arrows_from_qnet(debug_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunar_lander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
